{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hcostanog-AI/Racial-Bias-in-Abusive-Language-Detection/blob/main/Replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bf5e759-d8c7-437c-b7c6-45d3fda91e4b",
      "metadata": {
        "scrolled": true,
        "id": "3bf5e759-d8c7-437c-b7c6-45d3fda91e4b",
        "outputId": "197d7ecb-838e-45f7-81b4-1d359aa297a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/humbecosta/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/var/folders/0j/cwrs5bp96vb2kvhrr8xm3d_80000gn/T/ipykernel_99895/1093590421.py:33: DtypeWarning: Columns (26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, delimiter=';', on_bad_lines='skip')  # Load CSV using ';' delimiter\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File loaded successfully!\n",
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      4            6                   0        0      2   \n",
            "4           4      3            0                   2        1      1   \n",
            "\n",
            "                                               tweet  Unnamed: 7  Unnamed: 8  \\\n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...         NaN         NaN   \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...         NaN         NaN   \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...         NaN         NaN   \n",
            "3                    hut! We like it in our butt!\"\"\"         NaN         NaN   \n",
            "4  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...         NaN         NaN   \n",
            "\n",
            "  Unnamed: 9  ... Unnamed: 56 Unnamed: 57 Unnamed: 58 Unnamed: 59 Unnamed: 60  \\\n",
            "0        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
            "1        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
            "2        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
            "3        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
            "4        NaN  ...         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "  Unnamed: 61 Unnamed: 62 Unnamed: 63 Unnamed: 64 Unnamed: 65  \n",
            "0         NaN         NaN         NaN         NaN         NaN  \n",
            "1         NaN         NaN         NaN         NaN         NaN  \n",
            "2         NaN         NaN         NaN         NaN         NaN  \n",
            "3         NaN         NaN         NaN         NaN         NaN  \n",
            "4         NaN         NaN         NaN         NaN         NaN  \n",
            "\n",
            "[5 rows x 66 columns]\n",
            "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0               0      3            0                   0        3      2   \n",
            "1               1      3            0                   3        0      1   \n",
            "2               2      3            0                   3        0      1   \n",
            "3               3      4            6                   0        0      2   \n",
            "4               4      3            0                   2        1      1   \n",
            "...           ...    ...          ...                 ...      ...    ...   \n",
            "20485       20485      3            2                   1        0      0   \n",
            "20486       20486      3            0                   2        1      1   \n",
            "20487       20487      3            0                   3        0      1   \n",
            "20488       20488      6            0                   6        0      1   \n",
            "20489       20489      3            0                   0        3      2   \n",
            "\n",
            "                                                   tweet  Unnamed: 7  \\\n",
            "0      !!! RT @mayasolovely: As a woman you shouldn't...         NaN   \n",
            "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...         NaN   \n",
            "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...         NaN   \n",
            "3                        hut! We like it in our butt!\"\"\"         NaN   \n",
            "4      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...         NaN   \n",
            "...                                                  ...         ...   \n",
            "20485  you're such a retard i hope you get type 2 dia...         NaN   \n",
            "20486                     you's a muthaf***in lie &#8220         NaN   \n",
            "20487  young buck wanna eat!!.. dat nigguh like I ain...         NaN   \n",
            "20488              youu got wild bitches tellin you lies         NaN   \n",
            "20489  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...         NaN   \n",
            "\n",
            "       Unnamed: 8 Unnamed: 9  ... Unnamed: 56 Unnamed: 57 Unnamed: 58  \\\n",
            "0             NaN        NaN  ...         NaN         NaN         NaN   \n",
            "1             NaN        NaN  ...         NaN         NaN         NaN   \n",
            "2             NaN        NaN  ...         NaN         NaN         NaN   \n",
            "3             NaN        NaN  ...         NaN         NaN         NaN   \n",
            "4             NaN        NaN  ...         NaN         NaN         NaN   \n",
            "...           ...        ...  ...         ...         ...         ...   \n",
            "20485         NaN        NaN  ...         NaN         NaN         NaN   \n",
            "20486         NaN        NaN  ...         NaN         NaN         NaN   \n",
            "20487         NaN        NaN  ...         NaN         NaN         NaN   \n",
            "20488         NaN        NaN  ...         NaN         NaN         NaN   \n",
            "20489         NaN        NaN  ...         NaN         NaN         NaN   \n",
            "\n",
            "      Unnamed: 59 Unnamed: 60 Unnamed: 61 Unnamed: 62 Unnamed: 63 Unnamed: 64  \\\n",
            "0             NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "1             NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "2             NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "3             NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "4             NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "...           ...         ...         ...         ...         ...         ...   \n",
            "20485         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "20486         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "20487         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "20488         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "20489         NaN         NaN         NaN         NaN         NaN         NaN   \n",
            "\n",
            "      Unnamed: 65  \n",
            "0             NaN  \n",
            "1             NaN  \n",
            "2             NaN  \n",
            "3             NaN  \n",
            "4             NaN  \n",
            "...           ...  \n",
            "20485         NaN  \n",
            "20486         NaN  \n",
            "20487         NaN  \n",
            "20488         NaN  \n",
            "20489         NaN  \n",
            "\n",
            "[20490 rows x 66 columns]\n",
            "Column names: Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
            "       'class', 'tweet', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n",
            "       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n",
            "       'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17',\n",
            "       'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21',\n",
            "       'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25',\n",
            "       'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28', 'Unnamed: 29',\n",
            "       'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33',\n",
            "       'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37',\n",
            "       'Unnamed: 38', 'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41',\n",
            "       'Unnamed: 42', 'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45',\n",
            "       'Unnamed: 46', 'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49',\n",
            "       'Unnamed: 50', 'Unnamed: 51', 'Unnamed: 52', 'Unnamed: 53',\n",
            "       'Unnamed: 54', 'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57',\n",
            "       'Unnamed: 58', 'Unnamed: 59', 'Unnamed: 60', 'Unnamed: 61',\n",
            "       'Unnamed: 62', 'Unnamed: 63', 'Unnamed: 64', 'Unnamed: 65'],\n",
            "      dtype='object')\n",
            "Duplicates dropped successfully!\n",
            "Tweets preprocessed successfully!\n",
            "Best F1 score: 0.8716777391781682\n",
            "Best Parameters: {'clf__C': 1}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4608    0.4115    0.4348       243\n",
            "           1     0.9322    0.9138    0.9229      3040\n",
            "           2     0.7612    0.8580    0.8067       676\n",
            "\n",
            "    accuracy                         0.8735      3959\n",
            "   macro avg     0.7181    0.7278    0.7215      3959\n",
            "weighted avg     0.8741    0.8735    0.8731      3959\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hate     0.4608    0.4115    0.4348       243\n",
            "     Neither     0.7612    0.8580    0.8067       676\n",
            "   Offensive     0.9322    0.9138    0.9229      3040\n",
            "\n",
            "    accuracy                         0.8735      3959\n",
            "   macro avg     0.7181    0.7278    0.7215      3959\n",
            "weighted avg     0.8741    0.8735    0.8731      3959\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd                  # For data manipulation and loading CSVs\n",
        "import numpy as np                   # For numerical operations (was not used)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # For transforming text into numerical features\n",
        "from sklearn.linear_model import LogisticRegression          # Classifier used for training\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold  # For model validation and tuning\n",
        "from sklearn.metrics import f1_score, classification_report  # For evaluation metrics\n",
        "from sklearn.pipeline import Pipeline                        # To streamline feature extraction + model in one object\n",
        "import re                                                     # For regex-based text cleaning\n",
        "from nltk.stem import SnowballStemmer                        # For word stemming (reduces words to base form)\n",
        "from nltk.tokenize import TweetTokenizer                     # Tokenizer designed for tweets\n",
        "import nltk\n",
        "nltk.download('punkt')                                       # Download NLTK tokenizer model\n",
        "\n",
        "# Set up preprocessing tools\n",
        "stemmer = SnowballStemmer(\"english\")        # Initialize English stemmer\n",
        "tokenizer = TweetTokenizer()                # Initialize tweet-specific tokenizer\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_tweet(text):\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"<URL>\", text)         # Replace URLs with a placeholder\n",
        "    text = re.sub(r\"@[A-Za-z0-9_]+\", \"<MENTION>\", text)     # Replace mentions with a placeholder\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()                # Normalize whitespace\n",
        "    tokens = tokenizer.tokenize(text.lower())               # Lowercase and tokenize\n",
        "    stemmed = [stemmer.stem(token) for token in tokens]     # Stem each token\n",
        "    return \" \".join(stemmed)                                # Recombine tokens into a single string\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/Users/humbecosta/Desktop/UOB/Project AI GC/1st attempt/labeled_data.csv\"\n",
        "try:\n",
        "    df = pd.read_csv(file_path, delimiter=';', on_bad_lines='skip')  # Load CSV using ';' delimiter\n",
        "    print(\"File loaded successfully!\")\n",
        "    print(df.head())  # Preview first few rows\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"ParserError: {e}\")  # Catch formatting issues\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found: {file_path}\")  # Catch missing file errors\n",
        "\n",
        "print(df)  # Print entire DataFrame (useful for debugging, but can be large)\n",
        "\n",
        "# Check what columns are available\n",
        "print(\"Column names:\", df.columns)\n",
        "\n",
        "# Clean and preprocess tweets\n",
        "if 'tweet' in df.columns:\n",
        "    df = df.drop_duplicates(subset='tweet')  # Drop duplicate tweets\n",
        "    print(\"Duplicates dropped successfully!\")\n",
        "else:\n",
        "    print(\"Column 'tweet' not found in the dataframe.\")\n",
        "\n",
        "# Preprocess tweet text if column exists\n",
        "if 'tweet' in df.columns:\n",
        "    df['processed_tweet'] = df['tweet'].astype(str).apply(preprocess_tweet)  # Apply cleaning function\n",
        "    print(\"Tweets preprocessed successfully!\")\n",
        "else:\n",
        "    print(\"Column 'tweet' not found in the dataframe.\")\n",
        "\n",
        "\n",
        "# Feature extraction with TF-IDF\n",
        "# Use unigrams, bigrams, and trigrams\n",
        "# Limit to top 10,000 features to reduce dimensionality\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
        "\n",
        "# === Split data into train and validation sets ===\n",
        "X = df['processed_tweet']     # Feature input\n",
        "y = df['class']               # Labels (0 = Hate, 1 = Offensive, 2 = Neither)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,            # 80/20 split\n",
        "    stratify=y,               # Preserve label distribution in both sets\n",
        "    random_state=42           # Reproducibility\n",
        ")\n",
        "\n",
        "# === Build ML pipeline ===\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', vectorizer),                                        # First step: transform text into TF-IDF features\n",
        "    ('clf', LogisticRegression(solver='liblinear', class_weight='balanced'))  # Second step: train classifier\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning via grid search\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100]  # Regularization strengths to test\n",
        "}\n",
        "grid = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=StratifiedKFold(n_splits=5),    # Use stratified k-fold to ensure balanced folds\n",
        "    scoring='f1_weighted',             # Optimize for weighted F1 score\n",
        "    n_jobs=-1                          # Use all cores for faster training\n",
        ")\n",
        "grid.fit(X_train, y_train)             # Train and validate over grid\n",
        "\n",
        "# Show best results\n",
        "print(\"Best F1 score:\", grid.best_score_)       # Best score across folds\n",
        "print(\"Best Parameters:\", grid.best_params_)    # Corresponding hyperparameters\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_pred = grid.predict(X_val)  # Predict on held-out validation set\n",
        "print(classification_report(y_val, y_pred, digits=4))  # Print detailed precision/recall/F1\n",
        "\n",
        "# Convert label IDs to the proper categorize\n",
        "label_mapping = {0: \"Hate\", 1: \"Offensive\", 2: \"Neither\"}  # Label interpretation\n",
        "\n",
        "y_val_mapped = [label_mapping[label] for label in y_val]    # Map actual labels\n",
        "y_pred_mapped = [label_mapping[label] for label in y_pred]  # Map predictions\n",
        "\n",
        "# Print more readable classification report\n",
        "print(classification_report(y_val_mapped, y_pred_mapped, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec4fbc0d-45ea-4f3f-8cdd-8bc5eeebda96",
      "metadata": {
        "id": "ec4fbc0d-45ea-4f3f-8cdd-8bc5eeebda96"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}